{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import random\n",
    "import optax\n",
    "from functools import reduce\n",
    "\n",
    "# Functions\n",
    "import QM_Spin_Systems as QSS\n",
    "import LHV_Setup as LHV\n",
    "import Training as TRG\n",
    "import Plotting as PLT\n",
    "import Qudits as QDT\n",
    "\n",
    "key = random.PRNGKey(42) # initial key for random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize LHV model for single Werner state (runtime ~ 10 seconds on a single NVIDIA Quadro RTX 6000 GPU)\n",
    "\n",
    "N_spins = 2 # Number of spins, 2 for 2-spin Werner states\n",
    "\n",
    "# Measurement rules\n",
    "D = 5\n",
    "PLHV, hidden_dim = LHV.PLHV_sh(D) # Alternatives: PLHV_Bell = PLHV_sh(1), PLHV_sh3 = PLHV_sh(3), PLHV_sh5 = PLHV_sh(5), \n",
    "                                  #               PLHV_sh5_old (different normalization), PLHV_spherical_harmonics_planar (PLHV_sh5 w/ z=0)\n",
    "                                  #               PLHV_polynomial(D): Odd polynomial expansion up to degree D\n",
    "PQM = LHV.PQM_Werner # For Werner states, state=visibility\n",
    "                     # Alternatives: PQM (state given by its correlation matrix), \n",
    "                     #               PQM_XYZ (states with symmetries of \"XYZ Hamiltonian\" represented by correlators (xx, yy, zz))\n",
    "\n",
    "# State = Werner State with the following visibility\n",
    "visibility = .5\n",
    "\n",
    "# Split key\n",
    "key, key_init, key_train = random.split(key, (3, ))\n",
    "\n",
    "# Hyper parameters\n",
    "N_hidden = 2**14 # Hidden-variable cloud size\n",
    "N_measures = 2**8 # Batch size (number of tuples of measurement directions sampled per gradient descent step)\n",
    "N_measures_test = 2**14 # Batch size for final evaluation of the optimized cloud\n",
    "N_steps = 1*10**4 # Number of gradient descent steps\n",
    "hyper_params = [N_measures, N_measures_test, N_steps]\n",
    "\n",
    "# Optimizer\n",
    "N_steps_ft = 10**3 # Number of gradient descent steps with 10x smaller learning rate for fine tuning at the end of training\n",
    "learning_rate = N_hidden * 2e-5 # The learning rate should be proportional to the hidden-variable cloud size\n",
    "schedule = optax.piecewise_constant_schedule(learning_rate, boundaries_and_scales={N_steps-N_steps_ft: 0.1})\n",
    "optimizer = optax.adam(learning_rate=schedule) # Adam optimizer\n",
    "\n",
    "functions = [PLHV, PQM, LHV.KL, LHV.sample3Dprojective, optimizer] # Use the KL divergence as a distance measure and sample from all projective measurements\n",
    "# Alternatives for distance measure: L2 (squared difference), L1 (difference, not suitable for training)\n",
    "# Alternatives for sampling method: sample2Dprojective (only sample in xy plane), in general needs to be compatible with PQM\n",
    "\n",
    "# Init and optimize the hidden-variable cloud\n",
    "init = random.normal(key_init, (N_hidden, N_spins, hidden_dim)) # Independent gaussian initialization of all hidden-variable components in the cloud\n",
    "# Obtain optimized hidden-variable cloud, the progression of the loss during training and the final loss\n",
    "cloud, loss_values, test_loss = TRG.autoLHV(key_train, init, visibility, *hyper_params, *functions)\n",
    "\n",
    "print(\"Final loss (KL): {:.2e}\".format(test_loss))\n",
    "\n",
    "# Test mean difference of measurement probabilities ( = L1 loss)\n",
    "key, key_test = random.split(key)\n",
    "N_measures_test = 2**14\n",
    "test_functions = [PLHV, PQM, LHV.L1, LHV.sample3Dprojective]\n",
    "test_loss_L1 = TRG.test_loss(key_test, cloud, visibility, N_measures_test, *test_functions)\n",
    "print(\"Mean deviation (L1): {:.2e}\".format(test_loss_L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss progression\n",
    "dpi=150\n",
    "size=15\n",
    "stpsize = 10 # Only plot the loss every stpsize steps\n",
    "plt.figure(figsize=(6, 3), dpi = dpi)\n",
    "plt.plot(range(stpsize-1, N_steps, stpsize), loss_values[stpsize-1::stpsize], lw=.5, color=\"black\")\n",
    "plt.ylabel(r\"Loss\", size=size)\n",
    "plt.xlabel(r\"Steps\", size=size)\n",
    "plt.grid(alpha=0.5, zorder=-1)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim((stpsize-1, 1.5*N_steps))\n",
    "#plt.ylim((2e-8, 1e-7))\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize LHV model for an arbitrary qubit state defined by its correlations (runtime ~ 20 seconds on a single NVIDIA Quadro RTX 6000 GPU)\n",
    "\n",
    "N_spins = 1 # Number of qubits\n",
    "\n",
    "# Measurement rules\n",
    "PLHV, hidden_dim = LHV.PLHV_Bell() # = LHV.PLHV_sh(1)\n",
    "PQM = LHV.PQM # requires a state defined by its correlation matrix\n",
    "\n",
    "up = jnp.array([1., 0.])\n",
    "rho = jnp.outer(jnp.conjugate(up), up) # density matrix for the state \"up\"\n",
    "corrs = QSS.correlators(rho, N_spins) # correlation matrix\n",
    "\n",
    "key, key_init, key_train = random.split(key, (3, ))\n",
    "\n",
    "# Hyper parameters\n",
    "N_hidden = 2**12\n",
    "N_measures = 2**10\n",
    "N_measures_test = 2**14\n",
    "N_steps = 1*10**5\n",
    "hyper_params = [N_measures, N_measures_test, N_steps]\n",
    "\n",
    "# Optimizer\n",
    "N_steps_ft = 1*10**4\n",
    "learning_rate = N_hidden * 1e-5\n",
    "schedule = optax.piecewise_constant_schedule(learning_rate, boundaries_and_scales={N_steps-N_steps_ft: 0.1})\n",
    "optimizer = optax.adam(learning_rate=schedule)\n",
    "\n",
    "functions = [PLHV, PQM, LHV.KL, LHV.sample3Dprojective, optimizer]\n",
    "\n",
    "# Init and optimize\n",
    "init = random.normal(key_init, (N_hidden, N_spins, hidden_dim))\n",
    "cloud, loss_values, test_loss = TRG.autoLHV(key_train, init, corrs, *hyper_params, *functions)\n",
    "\n",
    "print(\"Final loss: {:.2e}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting including the hidden-variable distribution (first three components normalized to the surface of a sphere)\n",
    "import matplotlib\n",
    "\n",
    "dpi = 150\n",
    "size = 15\n",
    "stpsize = 10\n",
    "\n",
    "plt.figure(figsize=(5, 3*(N_spins)+3), dpi=dpi, constrained_layout=True)\n",
    "\n",
    "ax00 = plt.subplot(N_spins+1, 1, 1)\n",
    "ax00.plot(range(stpsize-1, N_steps, stpsize), loss_values[stpsize-1:N_steps:stpsize], lw=.5, color=\"black\")\n",
    "plt.xlabel(r\"Steps\", size=size-5)\n",
    "plt.grid(alpha=0.5, zorder=-1)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "N_grid = 100\n",
    "phi_grid = jnp.linspace(-jnp.pi, jnp.pi, 2*N_grid)\n",
    "theta_grid = jnp.linspace(-jnp.pi/2., jnp.pi/2., N_grid)\n",
    "phi_grid, theta_grid = jnp.meshgrid(phi_grid, theta_grid)\n",
    "normal_vectors = PLT.normal(phi_grid, theta_grid)\n",
    "delta = 0.02\n",
    "\n",
    "for j in range(N_spins):\n",
    "\n",
    "    spin = cloud[:, j, :3]\n",
    "    spin /= jnp.linalg.norm(spin, axis=-1, keepdims=True)\n",
    "\n",
    "    phi_grid_scatter, theta_grid_scatter = PLT.S2_angles(spin)\n",
    "\n",
    "    facecolors = PLT.gaussian_blurr(spin, normal_vectors, delta)\n",
    "    facecolors /= jnp.max(facecolors)\n",
    "    minimum = 0. # jnp.min(facecolors)\n",
    "    maximum = 1. # jnp.max(facecolors)\n",
    "    cnorm = matplotlib.colors.Normalize(vmin = minimum, vmax = maximum, clip=False)\n",
    "    facecolors = matplotlib.cm.plasma_r(cnorm(facecolors))\n",
    "\n",
    "    ax = plt.subplot(N_spins+1, 1, j+2, projection='mollweide')\n",
    "    c = ax.pcolormesh(phi_grid, theta_grid, facecolors, cmap=\"plasma_r\")\n",
    "    ax.scatter(phi_grid_scatter, theta_grid_scatter, s=1, color=\"black\", marker=\"o\", alpha=.3)\n",
    "    ax.set_xticks([-jnp.pi, -3.*jnp.pi/4., -jnp.pi/2, -jnp.pi/4., 0., jnp.pi/4., jnp.pi/2., 3.*jnp.pi/4., jnp.pi])\n",
    "    ax.set_yticks([-0.99*jnp.pi/2., -jnp.pi/4., 0., jnp.pi/4., 0.99*jnp.pi/2.])\n",
    "    ax.set_xticklabels(['', '', '', '', '', '', '', '', '']) #['$-\\pi$', '$-\\pi/2$', '0', '$\\pi/2$', '$\\pi$']\n",
    "    ax.set_yticklabels(['', '', '', '', '']) # ['$-\\pi/2$', '$-\\pi/4$', '0', '$\\pi/4$', '$\\pi/2$']\n",
    "    ax.set_longitude_grid_ends(90)\n",
    "    plt.grid(alpha=0.7)\n",
    "    ax.set_title(f\"Spin {j+1} Marginal\")\n",
    "\n",
    "plt.colorbar(c, ax=ax, location=\"bottom\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize LHV model for a qutrit state defined by its density matrix (runtime ~ 8,5 minutes on a single NVIDIA Quadro RTX 6000 GPU)\n",
    "\n",
    "# General Setup\n",
    "\n",
    "measurement_type = \"PVM\"    # PVM / POVM measurements\n",
    "d = 3                       # qudit dimension\n",
    "D = 1                       # maximal monomial degree\n",
    "symmetric = True            # Use permutation invariant measurement rule or not\n",
    "ONB = True                  # Orthonormalize monomials or not\n",
    "coeffs = None               # Coefficients for ONB from monomials (if previously saved)\n",
    "N_MC = 10**5                # Number of Monte Carlo samples for L2 inner product for Gram-Schmidt orthonormalization\n",
    "\n",
    "match measurement_type:\n",
    "    case \"PVM\":\n",
    "        delta = d # number of measurement outcomes\n",
    "        n_vars = (d-1) * (d**2-1) # number of variables specifying a measurement\n",
    "        sample = QDT.sample_PVMs # sample PVM measurements\n",
    "        params_extractor = QDT.gell_mann_params_PVM # extract Gell-Mann parameters from measurement operators\n",
    "        PQM = QDT.PQM_pvm # QM measurement rule\n",
    "    case \"POVM\":\n",
    "        delta = d**2\n",
    "        n_vars = (d**2-1) * d**2\n",
    "        sample = QDT.sample_POVMs\n",
    "        params_extractor = QDT.gell_mann_params_POVM\n",
    "        PQM = QDT.PQM_povm\n",
    "    case _:\n",
    "        print(\"Measurement type needs to be 'PVM' or 'POVM'\")\n",
    "\n",
    "if ONB and (coeffs is None):\n",
    "    key, key_M = random.split(key)\n",
    "    measurements = jnp.squeeze(sample(key_M, N_MC, 1, d)) # Monte Carlo Samples\n",
    "else:\n",
    "    measurements = None\n",
    "\n",
    "# Measurement rule, hidden-variable dimension and matrix of coefficients if monomials are orthonormalized\n",
    "PLHV, hidden_dim, coeffs = QDT.LHV_rule_constructor(delta, n_vars, D, params_extractor, symmetric=symmetric, ONB=ONB, coeffs=coeffs, samples=measurements, alpha=1e-4, beta=1e-3)\n",
    "\n",
    "\n",
    "# Hyper parameters\n",
    "N_hidden = 2**12   # hidden-variable cloud size\n",
    "N_steps = int(1e5)\n",
    "learning_rate = N_hidden * 3e-6\n",
    "N_measures = 2**9\n",
    "N_measures_test = 2**12\n",
    "N_test_runs = 2**4\n",
    "\n",
    "# Isotropic state\n",
    "N_particles = 2 # 2-qutrit state\n",
    "visibility = 0.1\n",
    "i = complex(0., 1.)\n",
    "basis = jnp.eye(d)\n",
    "psi = sum([reduce(jnp.kron, N_particles*[basis[j]]) for j in range(d)]) / jnp.sqrt(d) \n",
    "rho = visibility*jnp.outer(psi, psi) + (1.-visibility)*jnp.eye(d**N_particles)/d**N_particles\n",
    "\n",
    "# keys\n",
    "key, key_init, key_train = random.split(key, (3,))\n",
    "\n",
    "# loss function\n",
    "deviation = QDT.distance_KL\n",
    "\n",
    "N_steps1 = int(1e3) # linearly annihilate learning rate starting after N_step1 steps\n",
    "schedule = optax.schedules.linear_schedule(learning_rate, 0., N_steps-N_steps1, transition_begin=N_steps1)\n",
    "optimizer = optax.adam(learning_rate=schedule)\n",
    "hyper_params = [d, N_measures, N_measures_test, N_steps] \n",
    "functions = [PLHV, PQM, deviation, sample, optimizer]\n",
    "\n",
    "# Init hidden variables\n",
    "init = random.normal(key_init, (N_hidden, N_particles, delta, hidden_dim))\n",
    "\n",
    "print(\"Start optimizing LHV\")\n",
    "# optimize\n",
    "cloud, loss_values = QDT.autoLHV(key_train, init, rho, *hyper_params, *functions)\n",
    "\n",
    "print(\"Start evaluating LHV\")\n",
    "# test loss\n",
    "test_losses = np.zeros(N_test_runs)\n",
    "for jdx in range(N_test_runs):\n",
    "    key, key_test = random.split(key)\n",
    "    test_losses[jdx] = QDT.eval_test_loss(key_test, cloud, rho, d, N_measures_test, *(functions[:-1]))\n",
    "test_loss = np.mean(test_losses)\n",
    "\n",
    "print(\"final loss: {:.2e}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss progression\n",
    "dpi=150\n",
    "size=15\n",
    "stpsize = 10 # Only plot the loss every stpsize steps\n",
    "plt.figure(figsize=(6, 3), dpi = dpi)\n",
    "plt.plot(range(stpsize-1, N_steps, stpsize), loss_values[stpsize-1::stpsize], lw=.5, color=\"black\")\n",
    "plt.ylabel(r\"Loss\", size=size)\n",
    "plt.xlabel(r\"Steps\", size=size)\n",
    "plt.grid(alpha=0.5, zorder=-1)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim((stpsize-1, 1.5*N_steps))\n",
    "#plt.ylim((2e-8, 1e-7))\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
