{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Packages\n","import os\n","os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='.99' # Provide as much GPU memory as possible\n","\n","import numpy as np\n","import jax.numpy as jnp\n","import matplotlib.pyplot as plt\n","from jax import random\n","\n","# Import functions\n","import QM_Spin_Systems as QSS\n","import LHV_Training_Setup as LHV\n","import Training as TRG\n","import Plotting as PLT\n","\n","pi = jnp.pi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optimize LHV model for single Werner state (should run in a few seconds on a single GPU)\n","\n","N_spins = 2 # Number of spins, 2 for 2-spin Werner states\n","\n","# Measurement rules\n","D = 5\n","PLHV, hidden_dim = LHV.PLHV_sh(D) # Alternatives: PLHV_Bell = PLHV_sh(1), PLHV_sh3 = PLHV_sh(3), PLHV_sh5 = PLHV_sh(5), \n","                                  #               PLHV_sh5_old (different normalization), PLHV_spherical_harmonics_planar (PLHV_sh5 w/ z=0)\n","                                  #               PLHV_polynomial(D): Odd polynomial expansion up to degree D\n","PQM = LHV.PQM_Werner # For Werner states, state=visibility\n","                     # Alternatives: PQM (state given by its correlation matrix), \n","                     #               PQM_XYZ (states with symmetries of \"XYZ Hamiltonian\" represented by correlators (xx, yy, zz))\n","\n","# State\n","visibility = .5\n","\n","# Setup keys for random number generator\n","key = random.PRNGKey(0)\n","key, key_init, key_train = random.split(key, (3, ))\n","\n","# Hyper parameters\n","N_hidden = 2**14 # Hidden state cloud size\n","N_measures = 2**8 # Batch size (number of tuples of measurement directions sampled per gradient descent step)\n","N_measures_test = 2**14 # Batch size for final evaluation of the optimized cloud\n","N_steps = 1*10**4 # Number of gradient descent steps\n","N_steps_ft = 10**3 # Number of gradient descent steps with 10x smaller learning rate for fine tuning at the end of training\n","learning_rate = N_hidden * 2e-5 # The learning rate should be proportional to the hidden state cloud size\n","hyper_params = [N_measures, N_measures_test, N_steps, N_steps_ft, learning_rate]\n","\n","functions = [PLHV, PQM, LHV.KL, LHV.sample3Dprojective] # Use the KL divergence as a distance measure and sample from all projective measurements\n","# Alternatives for distance measure: L2 (squared difference), L1 (difference, not suitable for training)\n","# Alternatives for sampling method: sample2Dprojective (only sample in xy plane), in general needs to be compatible with PQM\n","\n","# Init and optimize the hidden state cloud\n","init = random.normal(key_init, (N_hidden, N_spins, hidden_dim)) # Independent gaussian initialization of all hidden state components in the cloud\n","# Obtain optimized hidden state cloud, the progression of the loss during training and the final loss\n","hidden_states, loss_values, test_loss = TRG.autoLHV(key_train, init, visibility, *hyper_params, *functions)\n","\n","print(\"Final loss: {:.2e}\".format(test_loss))\n","\n","# Test mean difference of measurement probabilities ( = L1 loss)\n","key, key_test = random.split(key)\n","N_measures_test = 2**14\n","test_functions = [PLHV, PQM, LHV.L1, LHV.sample3Dprojective]\n","test_loss_L1 = TRG.test_loss(key_test, hidden_states, visibility, N_measures_test, *test_functions)\n","print(\"Mean deviation: {:.2e}\".format(test_loss_L1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the loss progression\n","dpi=150\n","size=15\n","stpsize = 10 # Only plot the loss every stpsize steps\n","plt.figure(figsize=(6, 3), dpi = dpi)\n","plt.plot(range(stpsize-1, N_steps, stpsize), loss_values[stpsize-1::stpsize], lw=.5, color=\"black\")\n","plt.ylabel(r\"Loss\", size=size)\n","plt.xlabel(r\"Steps\", size=size)\n","plt.grid(alpha=0.5, zorder=-1)\n","plt.xscale(\"log\")\n","plt.xlim((stpsize-1, 1.5*N_steps))\n","#plt.ylim((2e-8, 1e-7))\n","plt.yscale(\"log\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optimize LHV model for an arbitrary state defined by its correlations\n","\n","N_spins = 1\n","\n","# Measurement rules\n","PLHV, hidden_dim = LHV.PLHV_Bell() # = LHV.PLHV_sh(1)\n","PQM = LHV.PQM # requires a state defined by its correlation matrix\n","\n","up = jnp.array([1., 0.])\n","rho = jnp.outer(jnp.conjugate(up), up) # density matrix for the state \"up\"\n","corrs = QSS.correlators(rho, N_spins) # correlation matrix\n","\n","key = random.PRNGKey(0)\n","key, key_init, key_train = random.split(key, (3, ))\n","\n","# Hyper parameters\n","N_hidden = 2**10 # smaller N_h less accurate by nicer for visualization below\n","N_measures = 2**12\n","N_measures_test = 2**14\n","N_steps = 3*10**4\n","N_steps_ft = 10**3\n","learning_rate = N_hidden * 2e-5\n","hyper_params = [N_measures, N_measures_test, N_steps, N_steps_ft, learning_rate]\n","functions = [PLHV, PQM, LHV.KL, LHV.sample3Dprojective] # Use the squared difference for the loss function\n","\n","# Init and optimize\n","init = random.normal(key_init, (N_hidden, N_spins, hidden_dim))\n","hidden_states, loss_values, test_loss = TRG.autoLHV(key_train, init, corrs, *hyper_params, *functions)\n","\n","print(\"Final loss: {:.2e}\".format(test_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting including the hidden state distribution (first three components normalized to the surface of a sphere)\n","import matplotlib\n","\n","dpi = 150\n","size = 15\n","stpsize = 10\n","\n","plt.figure(figsize=(5, 3*(N_spins)+3), dpi=dpi, constrained_layout=True)\n","\n","ax00 = plt.subplot(N_spins+1, 1, 1)\n","ax00.plot(range(stpsize-1, N_steps, stpsize), loss_values[stpsize-1:N_steps:stpsize], lw=.5, color=\"black\")\n","plt.title(r\"Final loss $= {:.2e}$\".format(test_loss), size=size-5)\n","plt.xlabel(r\"Steps\", size=size-5)\n","plt.grid(alpha=0.5, zorder=-1)\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","\n","N_grid = 100\n","phi_grid = jnp.linspace(-pi, pi, 2*N_grid)\n","theta_grid = jnp.linspace(-pi/2., pi/2., N_grid)\n","phi_grid, theta_grid = jnp.meshgrid(phi_grid, theta_grid)\n","normal_vectors = PLT.normal(phi_grid, theta_grid)\n","delta = 0.02\n","\n","for j in range(N_spins):\n","\n","    spin = hidden_states[:, j, :3]\n","    spin /= jnp.linalg.norm(spin, axis=-1, keepdims=True)\n","\n","    phi_grid_scatter, theta_grid_scatter = PLT.S2_angles(spin)\n","\n","    facecolors = PLT.gaussian_blurr(spin, normal_vectors, delta)\n","    facecolors /= jnp.max(facecolors)\n","    minimum = 0. # jnp.min(facecolors)\n","    maximum = 1. # jnp.max(facecolors)\n","    cnorm = matplotlib.colors.Normalize(vmin = minimum, vmax = maximum, clip=False)\n","    facecolors = matplotlib.cm.plasma_r(cnorm(facecolors))\n","\n","    ax = plt.subplot(N_spins+1, 1, j+2, projection='mollweide')\n","    c = ax.pcolormesh(phi_grid, theta_grid, facecolors, cmap=\"plasma_r\")\n","    ax.scatter(phi_grid_scatter, theta_grid_scatter, s=1, color=\"black\", marker=\"o\", alpha=1)\n","    ax.set_xticks([-pi, -3.*pi/4., -pi/2, -pi/4., 0., pi/4., pi/2., 3.*pi/4., pi])\n","    ax.set_yticks([-0.99*pi/2., -pi/4., 0., pi/4., 0.99*pi/2.])\n","    ax.set_xticklabels(['', '', '', '', '', '', '', '', '']) #['$-\\pi$', '$-\\pi/2$', '0', '$\\pi/2$', '$\\pi$']\n","    ax.set_yticklabels(['', '', '', '', '']) # ['$-\\pi/2$', '$-\\pi/4$', '0', '$\\pi/4$', '$\\pi/2$']\n","    ax.set_longitude_grid_ends(90)\n","    plt.grid(alpha=0.7)\n","    ax.set_title(f\"Spin {j+1} Marginal\")\n","\n","plt.colorbar(c, ax=ax, location=\"bottom\")\n","plt.show()"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
